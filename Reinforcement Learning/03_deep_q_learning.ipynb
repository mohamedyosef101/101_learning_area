{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/101_learning_area/blob/area/Reinforcement%20Learning/03_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Source:** [huggingface.co](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)"
      ],
      "metadata": {
        "id": "YtBYkYIjb3Ji"
      },
      "id": "YtBYkYIjb3Ji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "f5RweVjncImz"
      },
      "id": "f5RweVjncImz"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium pygame pyglet pickle5 pyyaml pyvirtualdisplay\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/simoninithomas/gym-games"
      ],
      "metadata": {
        "id": "pHIpd5Y5cCdO"
      },
      "id": "pHIpd5Y5cCdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "FuoQ8lOa3gEB"
      },
      "id": "FuoQ8lOa3gEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the packages"
      ],
      "metadata": {
        "id": "oXmecP8F3_KE"
      },
      "id": "oXmecP8F3_KE"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical as cat\n",
        "\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "import imageio"
      ],
      "metadata": {
        "id": "tZ6EISl94Cxf"
      },
      "id": "tZ6EISl94Cxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU"
      ],
      "metadata": {
        "id": "oLh0Mvo-3_AI"
      },
      "id": "oLh0Mvo-3_AI"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PJK7d9DY5Kzk"
      },
      "id": "PJK7d9DY5Kzk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CartPole-v1"
      ],
      "metadata": {
        "id": "ExfeX4V_5sJ6"
      },
      "id": "ExfeX4V_5sJ6"
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "\n",
        "# Get the state space and action space\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "print(f\"\"\"The State Space is: {states}\n",
        "      \\nThe Action Space is: {actions}\"\"\")"
      ],
      "metadata": {
        "id": "ThJ_CdOj513P"
      },
      "id": "ThJ_CdOj513P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy"
      ],
      "metadata": {
        "id": "i0RjW_kl8LJI"
      },
      "id": "i0RjW_kl8LJI"
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, states, actions, horizon):\n",
        "    super(Policy, self).__init__()\n",
        "\n",
        "    # two fully connected layers\n",
        "    self.fc1 = nn.Linear(states, horizon)\n",
        "    self.fc2 = nn.Linear(horizon, actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2\n",
        "    return F.softmax(x, dim=1)\n",
        "\n",
        "  def act(self, state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    probs = self.forward(state).cpu()\n",
        "    m = cat(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(actions)"
      ],
      "metadata": {
        "id": "vKNNccvc8Pwl"
      },
      "id": "vKNNccvc8Pwl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for mistakes\n",
        "debug_policy = Policy(states, actions, 64).to(device)\n",
        "debug_policy.act(env.reset())"
      ],
      "metadata": {
        "id": "nE-88su096O0"
      },
      "id": "nE-88su096O0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforce Algorithm"
      ],
      "metadata": {
        "id": "M0n8UD4x_Yxp"
      },
      "id": "M0n8UD4x_Yxp"
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer,\n",
        "              n_training_episodes, max_t, gamma, print_every):\n",
        "  # calculate the score\n",
        "  scores_deque = deque(maxlen=100)\n",
        "  scores = []\n",
        "\n",
        "  for episode in range(1, n_training_episodes + 1):\n",
        "    saved_log_probs = []\n",
        "    rewards = []\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(max_t):\n",
        "      action, log_prob = policy.act(state)\n",
        "      saved_log_probs.append(log_prob)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      if done:\n",
        "        break\n",
        "    scores_deque.append(sum(rewards))\n",
        "    scores.append(rewards)\n",
        "\n",
        "    returns = deque(maxlen=max_t)\n",
        "    n_steps = len(rewards)\n",
        "\n",
        "    for t in range(n_steps)\n"
      ],
      "metadata": {
        "id": "y50Aa1zr_cPo"
      },
      "id": "y50Aa1zr_cPo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}