{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/101_learning_area/blob/area/Reinforcement%20Learning/03_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Policy** Gradient"
      ],
      "metadata": {
        "id": "IZO77o0EwUqZ"
      },
      "id": "IZO77o0EwUqZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Source:** [huggingface.co](https://huggingface.co/learn/deep-rl-course/unit4/hands-on)"
      ],
      "metadata": {
        "id": "YtBYkYIjb3Ji"
      },
      "id": "YtBYkYIjb3Ji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "f5RweVjncImz"
      },
      "id": "f5RweVjncImz"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium pygame pyglet pickle5 pyyaml pyvirtualdisplay\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/simoninithomas/gym-games"
      ],
      "metadata": {
        "id": "pHIpd5Y5cCdO"
      },
      "id": "pHIpd5Y5cCdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "FuoQ8lOa3gEB",
        "outputId": "c51d2b4a-6719-494c-888a-fc6a73d97a8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FuoQ8lOa3gEB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ca1a0315180>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the packages"
      ],
      "metadata": {
        "id": "oXmecP8F3_KE"
      },
      "id": "oXmecP8F3_KE"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical as cat\n",
        "\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "import imageio\n",
        "\n",
        "print(\"Packages are ready!\")"
      ],
      "metadata": {
        "id": "tZ6EISl94Cxf",
        "outputId": "1de4016f-0423-48fd-d673-5592ec1954a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tZ6EISl94Cxf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU"
      ],
      "metadata": {
        "id": "oLh0Mvo-3_AI"
      },
      "id": "oLh0Mvo-3_AI"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PJK7d9DY5Kzk",
        "outputId": "a04d1e85-e055-446e-d7cd-b2b4491c0ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PJK7d9DY5Kzk",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CartPole-v1"
      ],
      "metadata": {
        "id": "ExfeX4V_5sJ6"
      },
      "id": "ExfeX4V_5sJ6"
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "\n",
        "# Get the state space and action space\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "print(f\"\"\"The State Space is: {states}\n",
        "      \\nThe Action Space is: {actions}\"\"\")"
      ],
      "metadata": {
        "id": "ThJ_CdOj513P",
        "outputId": "0bd57349-c9bb-4505-af7c-ed8e89850234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ThJ_CdOj513P",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The State Space is: 4\n",
            "      \n",
            "The Action Space is: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy"
      ],
      "metadata": {
        "id": "i0RjW_kl8LJI"
      },
      "id": "i0RjW_kl8LJI"
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, states, actions, horizon):\n",
        "    super(Policy, self).__init__()\n",
        "\n",
        "    # two fully connected layers\n",
        "    self.fc1 = nn.Linear(states, horizon)\n",
        "    self.fc2 = nn.Linear(horizon, actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x, dim=1)\n",
        "\n",
        "  def act(self, state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    probs = self.forward(state).cpu()\n",
        "    m = cat(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "vKNNccvc8Pwl"
      },
      "id": "vKNNccvc8Pwl",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_policy = Policy(states, actions, 64).to(device)\n",
        "debug_policy.act(env.reset())"
      ],
      "metadata": {
        "id": "UkROY3gJr4QF",
        "outputId": "609ea058-cc7a-42b7-c839-eb319bb2881a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UkROY3gJr4QF",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, tensor([-0.6695], grad_fn=<SqueezeBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforce Algorithm"
      ],
      "metadata": {
        "id": "M0n8UD4x_Yxp"
      },
      "id": "M0n8UD4x_Yxp"
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer,\n",
        "              n_training_episodes, max_t, gamma, print_every):\n",
        "  # calculate the score\n",
        "  scores_deque = deque(maxlen=100)\n",
        "  scores = []\n",
        "\n",
        "  for episode in range(1, n_training_episodes + 1):\n",
        "    saved_log_probs = []\n",
        "    rewards = []\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(max_t):\n",
        "      action, log_prob = policy.act(state)\n",
        "      saved_log_probs.append(log_prob)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      if done:\n",
        "        break\n",
        "    scores_deque.append(sum(rewards))\n",
        "    scores.append(rewards)\n",
        "\n",
        "    returns = deque(maxlen=max_t)\n",
        "    n_steps = len(rewards)\n",
        "\n",
        "    for t in range(n_steps)[::-1]:\n",
        "      disc_return_t = returns[0] if len(returns) > 0 else 0\n",
        "      returns.appendleft(gamma * disc_return_t + rewards[t])\n",
        "\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    policy_loss = []\n",
        "    for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "      policy_loss.append(-log_prob * disc_return)\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if episode % print_every == 0:\n",
        "      print(f\"Episode {episode} \\tAverage Score: {np.mean(scores_deque):.2f}\")"
      ],
      "metadata": {
        "id": "y50Aa1zr_cPo"
      },
      "id": "y50Aa1zr_cPo",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Train the Reinforce"
      ],
      "metadata": {
        "id": "VPqnz24ykYIp"
      },
      "id": "VPqnz24ykYIp"
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"horizon\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": states,\n",
        "    \"action_space\": actions,\n",
        "}"
      ],
      "metadata": {
        "id": "ARHw5VOPkbJK"
      },
      "id": "ARHw5VOPkbJK",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create policy and add it to the device\n",
        "\n",
        "cartpole_policy = Policy(\n",
        "    cartpole_hyperparameters[\"state_space\"],\n",
        "    cartpole_hyperparameters[\"action_space\"],\n",
        "    cartpole_hyperparameters[\"horizon\"],\n",
        ").to(device)\n",
        "\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(),\n",
        "                                lr=cartpole_hyperparameters[\"lr\"])"
      ],
      "metadata": {
        "id": "CJY2FgdilJpp"
      },
      "id": "CJY2FgdilJpp",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = reinforce(\n",
        "    cartpole_policy,\n",
        "    cartpole_optimizer,\n",
        "    cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "    cartpole_hyperparameters[\"max_t\"],\n",
        "    cartpole_hyperparameters[\"gamma\"],\n",
        "    100,\n",
        ")"
      ],
      "metadata": {
        "id": "6czkYzFrmJuX",
        "outputId": "6bd30d77-b452-4859-93e4-3872acd4a061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6czkYzFrmJuX",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 \tAverage Score: 314.48\n",
            "Episode 200 \tAverage Score: 461.93\n",
            "Episode 300 \tAverage Score: 255.23\n",
            "Episode 400 \tAverage Score: 378.78\n",
            "Episode 500 \tAverage Score: 500.00\n",
            "Episode 600 \tAverage Score: 500.00\n",
            "Episode 700 \tAverage Score: 500.00\n",
            "Episode 800 \tAverage Score: 500.00\n",
            "Episode 900 \tAverage Score: 500.00\n",
            "Episode 1000 \tAverage Score: 422.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Method"
      ],
      "metadata": {
        "id": "-OedJ9VUmrmy"
      },
      "id": "-OedJ9VUmrmy"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "zgS0nxpWmwFa"
      },
      "id": "zgS0nxpWmwFa",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(eval_env, cartpole_hyperparameters[\"max_t\"],\n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ],
      "metadata": {
        "id": "7Sznn7zPoVVi",
        "outputId": "32aa3154-d4ca-409e-92e5-3ac1ad957f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7Sznn7zPoVVi",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
